---
title: "MathLab - Lecture 6"
output:   learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(interactions)
knitr::opts_chunk$set(echo = FALSE)
mtcars$am <- factor(mtcars$am, labels = c("automatic", "manual"))
mtcars$vs <- factor(mtcars$vs, labels = c("V", "S"))
mtcars$cyl <- factor(mtcars$cyl)
mtcars$gear <- factor(mtcars$gear)
mtcars$carb <- factor(mtcars$carb)
library(ggplot2)
mod_full <- lm(mpg ~ . , data = mtcars)
```

## Linear Regression

Let's consider again the `mtcars` dataset we have been extensively using throughout the module. To start with let's call the function `str` over `mtcars` to review the variables in the dataset.

```{r str, exercise = TRUE}
# Call the function str

```

As you can see the variables `am`, `vs`, `cyl`, `gear` and `carb` are coded as factors. I achieved this using the following code.

```{r mtcars , echo = TRUE, exercise = FALSE}
mtcars$am <- factor(mtcars$am, labels = c("automatic", "manual"))
mtcars$vs <- factor(mtcars$vs, labels = c("V", "S"))
mtcars$cyl <- factor(mtcars$cyl)
mtcars$gear <- factor(mtcars$gear)
mtcars$carb <- factor(mtcars$carb)
```

We want to predict `mpg` using all other explanatory variables included in the dataset. In R this can be easily done using the function `lm`. For example 

```{r lm, echo = TRUE, eval = FALSE, exercise = FALSE}
lm(mpg ~ hp + cyl, data = mtcars)
```
fits a linear regression model where `mpg` is the response variable, `hp` and `cyl` are explanatory variables, and these variables are included in the dataframe `mtcars`. The syntax should look somewhat familiar since it is the one we discussed during model building.

Often we may want to start our analysis fitting a linear regression where all available explanatory variables are used in the model. R allows us to do this conveniently since we can just put a `.` on the right hand side of the tilde. Let's try and see the output of this model.

```{r mod1, exercise = TRUE, exercise.lines = 6}
# Fit a model with all explanatory variables and store it in mod_full
mod_full <- lm(mpg ~ . , data = mtcars)

# Print the object mod_full by simply calling the name of the variable

```

We see that it first tells us the formula we input into R as well as the coefficients of the model.

The “thing” that is returned by the `lm()` function and that we stored in `mod_full` is actually an object of class `lm` which is a list. The exact details of this are unimportant unless you are seriously interested in the inner-workings of R, but it is relevant to know how to access this information. There are various functions we can call over objects of class `lm`:

 - `coef`: returns the coefficients of the model
 - `resid`: returns the residuals/errors of the model
 - `fitted`: returns the predicted/fitted/model values

```{r func, exercise = TRUE, exercise.lines = 9}
# Return the coefficents of mod_full
coef(mod_full)

# Return the residuals of mod_full


# Return the fitted values of mod_full
```

## The Summary Function

An R function that is useful in many situations is `summary()`. You actually have already seen this function: when called over a dataframe it returns some numerical summaries of the variables. We see that when it is called on our model, it returns a good deal of information.

```{r summary, exercise = TRUE, exercise.lines = 3}
# Call the function summary over the variable mod_full

```

You should recognize this output!!! This is what we have seen multiple times already in class which gives a lot of information about our model: 

1. First, it tells us which model it is showing the summary of.
2. Second, some numerical summaries of the distribution of the residuals/erros
3. Third, a table with the value of the coefficients as well as the p-value of the associated test of hypothesis
4. Fourth, a line with the so-called residual standard error (another type of error we did not introduce)
5. Fifth, the $R^2$ and adj-$R^2$ indexes
6. Last, the p-value of the test of hypothesis to check if the model is different from a model with the intercept only.

## More on Using lm

### Prediction

A function we will often use is `predict`. This is used to predict values of the response using our model. 

```{r pred, exercise = TRUE, exercise.lines = 3}
# Predict the response at the values of the predictors in our dataset
predict(mod_full, newdata = mtcars)
```

So we are predicting the response for the observations we had in our dataset. This actually coincides with the fitted values that we can obtain with `fitted`. More often we may want to know what our model would predict at values of the predictors we did not observe.

```{r pred2, exercise = TRUE}
# First we define a new possible value of explanatory variables
newdata <- data.frame(cyl = "6", disp = 230, hp = 150, drat = 4, wt = 3, qsec = 18, vs = "V" , am = "manual", gear = "4" , carb = "3")

# We then predict the response
predict(mod_full, newdata = newdata)
```

Notice that here we simply gave as input to `newdata` just one potential new observation, but this was stored in a dataframe. So we could equally give as input any dataframe with an arbitrary number of rows/observations.

### Confidence Intervals

We can also easily construct confidence intervals for the parameters of the linear regression as well as for predicted values of the response. In the first case, we simply use the function `confint` over an object of class `lm`. One if its arguments `level` fixes the confidence level (e.g 0.95, 0.99 etc).

```{r conf, exercise = TRUE, exercise.lines = 3}
# Construct a 0.99 confidence interval for the parameters of mod_full
```

To construct an confidence interval for the predicted response we need to specify the argument `interval = "confidence"` and `level` in the function `predict`, as exemplified below.

```{r conf1, exercise = TRUE, exercise.lines =6}
# Create an observation to be predicted
newdata <- data.frame(cyl = "6", disp = 230, hp = 150, drat = 4, wt = 3, qsec = 18, vs = "V" , am = "manual", gear = "4" , carb = "3")

# Confidence interval for the response
predict(mod_full, newdata, interval = "confidence", level = 0.95)
```

## Interactions

In class we have discussed the possibility of adding interactions to our model. In the model building lecture we defined the notation `X1:X2` to denote the interaction between two variables `X1` and `X2`. Furthermore, recall that if an interaction is included in a model, then also the two main terms should be included. This means that

 - `Y ~ X1 + X2 + X1:X2` is an appropriate model, but
 - `Y ~ X1 + X1:X2` is not. 
 
Since whenever an interaction is in a model there should also be the main terms, there is a shortcut to write the appropriate model which is `Y ~ X1*X2`: this automatically includes all relevant terms. Let's give it a try.

```{r int, exercise = TRUE, exercise.lines = 5}
# Let's construct a model with interaction
mod_int <- lm(mpg ~ cyl*hp+ wt*qsec, data = mtcars)

# Report a summary of mod_int

```

So it seems that overall the interactions are not significant. One way to check interactions is by creating the interaction plot. A nice package to create such plots is the `interactions` package. In order to run the following code you need it installed in your computer.

```{r int1, exercise = TRUE, exercise.lines = 10}
# First define the model
mod_int <- lm(mpg ~ cyl*hp+ wt*qsec, data = mtcars)

# Call the interaction package
library(interactions)

# Interaction plot
interact_plot(mod_int,hp,cyl)
```

The function `interact_plot` takes as first input the model (where the interaction is fitted), it then takes the variable that will be plotted on the x-axis, and as last input we have the variable used to modulate the relationship between the response and the second input. Although we can see that for 4-cylinder cars as `hp` increases `mpg` decreases much more rapidly, such a difference does not seem to be statistically significant.

Let's look at the interaction plot between two continuous variables.

```{r int2, exercise = TRUE, exercise.lines = 3 }
mod_int <- lm(mpg ~ cyl*hp+ wt*qsec, data = mtcars)
interact_plot(mod_int,wt,qsec)
```
Here the variable over which we are modulating, which is continuous, is considered at the mean value, and at $\pm 1$ standard deviation from the mean. The interpretation is however the same: the interaction is more likely to be significant when the lines differ in slope.

## Model's Assessment

In class we discussed the need to check if the assumptions of a model are met. In R we can quickly produce the most important graphs to this purpose by simply using the function `plot` over the `lm` object `mod_full`.

```{r plot, exercise = TRUE, exercise.lines = 3}
# Call the function plot over mod_full

```
You should be familiar with the first two plots:

 - The first is the residuals vs fitted values. Here we should not see any pattern or change in variance in the points. They should look like a random cloud.
 - The second is the Normal qq-plot. Points should fall in a straight line in order to assume normality of the errors.
 
The third plot is a variation of the residuals vs fitted (you should not worry about this). The fourth is used to check for outliers. The red lines you see are Cook's distances equal to 0.5 and 1: points beyond those lines may be influential observations. An alternative plot to check for influential observations can be created as follows.

```{r inf, exercise = TRUE, exercise.lines =2}
plot(mod_full,4)
```
This gives a clearer idea of which observations may be influential.

If you want to produce the residuals vs fitted plot only you can use the code `plot(mod_full,1)` and for the qqplot `plot(mod_full,2)`.

The plots above are quite basic and if you want to create your own nicer version, you can create a data.frame with the relevant variables (e.g. fitted, resid, etc) and then use ggplot for instance.

## Transformations

When the assumptions of linear regression are not met, one possible solution is to apply a transformation to the response, most often the log transformation. This can be easily done in R by simply writing `log(y)` in the model if `y` is the response.

Let's look at the code below and the diagnostic plots.

```{r trans, exercise = TRUE, exercise.lines = 6}
# Let's fit the model with the log response
transformed_model <- lm(log(mpg) ~ . , data = mtcars)

# Diagnostic plots
plot(transformed_model)
```

## Multicollinearity

In class we discussed multicollinearity. In order to compute the variance inflation factor we can use for instance the package `car` and its function `vif` (in order to run the code below, you need to have the package `car` installed).

```{r car, exercise = TRUE, exercise.lines = 4}
library(car)
car::vif(mod_full)
```

As you can see the output is slightly different from what we saw in class. The reason being that we also have categorical explanatory variables so that R produces a generalized VIF (GVIF). As a recommendation use the third column of this output and a stricter rule of thumb - for instance over 5 can be considered already quite large.

Another good alternative is to check for multicollinearity only between continuous variables. In order to do this, you would need to (i) create a linear model with the function `lm` where explanatory variables are only continuous (ii) call the function `vif` over this new model.

Notice that in the code above I didn't first load the `car` package and then used the `vif` function. If a package is installed in the computer we can access its functions with the `::` as above. `car::vif` calls the function `vif` from `car` but without actually loading `car`.

## Model selection

In the last class we discussed tools to select a model. 

 - One way is to start with the full model and one at a time eliminate variables which are not significant until you only have significant ones. 
 
 - We also saw forward, backward and stepwise model selection which automatically search over the space of all possible models.

These routines can be implemented in R using the function `regsubsets` from the `leaps` package. You can look for more details [here](http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/154-stepwise-regression-essentials-in-r/).

Notice also that there are functions `AIC` and `BIC` which if called over a `lm` object as `mod_full` give you the value of AIC and BIC.
